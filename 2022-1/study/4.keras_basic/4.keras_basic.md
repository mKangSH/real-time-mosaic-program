# Neural network

It has more than one layer containing one more neuron(node).

Inside of neuron, weight that is updated by model training exists.

# Dense layer(Fully Connected Layer)

Status that all neuron interconnect themselves between each layer.

Be able to direct activation function for activation parameter.

Input value : 1 rank tensor

# Flatten layer

It changes multidimensional data to 1 rank tensor.

Use tf.keras.layers.Flatten() function.

# Activation Function

(_image_) not found

It changes input to nonlinear output.

ex) sigmoid, tanh(Hyperbolic Tangent), ReLU(Rectified Unit), Leaky ReLU, etc.
  - Use the 'sigmoid' activation function of output node if there is only one output node.
  - Use the 'softmax' activation function of output node if there are more than one output node.

[Activation Function document](https://www.tensorflow.org/api_docs/python/tf/keras/activations)

# Optimizer

Method changing neural network setting such as weights and lr(learning rate).

Keras supports SGD, Adam, Adagrad, Nadam, RMSprop, Adadelta, Adamax, Ftrl.

[Optimizer list document](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)

# Metrics

To evaluate classification model.

ex) 'accuracy'('acc'), 'auc', 'precision', 'recall'

[Metrics list document](https://www.tensorflow.org/api_docs/python/tf/keras/metrics)

# Model detailed setting
- __Initial value setting__
  - Method to initialize each layer.
  - [Initializer value document](https://www.tensorflow.org/api_docs/python/tf/keras/initializers)

- __Regularization__
  - Method to prevent from overfitting.
  - [Regularizer document](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers)

- __Dropout__
  - The way to unlink randomly several node.
  - tf.keras.layers.Dropout() function. parameter is ratio of node to unlink.

- __Batch Normalization__
  - Before the data pass to activation function, this mehod normalizes scale of mini batch.
  - It decreases loss rate but increses model compile time.

# Callback

_description_
